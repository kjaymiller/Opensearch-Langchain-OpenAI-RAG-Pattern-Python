{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Using Langchain to create a custom prompt\n",
    "\n",
    "We've taken our data and used knn search to find articles close to our topic. Now we can use that information to help provide insight to our LLM and use that to help provide context to our chat bot.\n",
    "\n",
    "Let's run our search from the last notebook again. \n",
    "\n",
    "> **Reminder** For this data we used _OpenAI_ but our embeddings but this could we could have used any embedding algorithm. You can find other options on https://huggingface.co and similar websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "index_name = \"openai_wikipedia_index\"\n",
    "connection_string = os.getenv(\"OPENSEARCH_SERVICE_URI\")\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "opensearch_client = OpenSearch(connection_string, use_ssl=True, timeout=100)\n",
    "\n",
    "# Define model\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define the Client\n",
    "openaiclient = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "def search(\n",
    "        embedding:list[float],\n",
    "        index_name:str=index_name,\n",
    "        num_results:int=5,\n",
    "        k_distance:int=10\n",
    ") -> str:\n",
    "    \"\"\"Preform a search using the embedding and return the top 5 results.\"\"\"\n",
    "\n",
    "    return opensearch_client.search(\n",
    "        index = index_name,\n",
    "        body = {\n",
    "            \"size\": num_results,\n",
    "            \"query\" : {\n",
    "                \"knn\" : {\n",
    "                    \"content_vector\":{\n",
    "                        \"vector\": embedding,\n",
    "                        \"k\": k_distance,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Define question\n",
    "traditional_topping = 'pepperoni'\n",
    "untraditional_topping = 'maraschio cherries'\n",
    "attempt_to_trick = \"Where are my tax benefits?\"\n",
    "\n",
    "questions = [traditional_topping, untraditional_topping, attempt_to_trick]\n",
    "# Create embedding\n",
    "\n",
    "for topping in questions:\n",
    "    question_embedding = openaiclient.embeddings.create(input=topping + \"pizza toppings\", model=EMBEDDING_MODEL).data[0].embedding\n",
    "    docs = search(question_embedding)\n",
    "    print(f\"Results for '{topping} pizza toppings':\")\n",
    "    print(\"\\n\".join([doc['_source']['title'] for doc in docs['hits']['hits']]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# llm = ChatOllama()\n",
    "\n",
    "\n",
    "top_hit_summary = [doc['_source']['text'] for doc in opensearch_response[\"hits\"][\"hits\"]]\n",
    "\n",
    "template=\"\"\"\n",
    "You are an assistant for a school that trains Pizza restaurant franchise owners and chefs.\n",
    "\n",
    "Your goal is to helpo them find both traditional and unique ingredients for their pizza recipes.\n",
    "\n",
    "Your suggestions should be based on the following criteria:\n",
    " - Ingredients should be able to be added to the pizza as a topping or sauce\n",
    " - Ingredients should be able to be consumed by humans\n",
    " - Flavor profiles should be compatible with pizza\n",
    "\n",
    "Comment on their potential taste, texture, and how they might pair with other ingredients.\n",
    "\n",
    "Respond to this Question: {question}\n",
    "\n",
    "Don't suggest other ingredients.\n",
    "\n",
    "Use ONLY the data below to influence your answer\n",
    "{data}\n",
    "\n",
    "Keep your answer to limited to no more than 2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"data\": RunnablePassthrough()} | prompt | llm\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"question\": question, \"data\": top_hit_summary})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "connection_string = os.getenv(\"OPENSEARCH_SERVICE_URI\")\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(connection_string, use_ssl=True, timeout=100)\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run semantic search queries with LangChain and OpenSearch\n",
    "\n",
    "With the above embedding calculated, we can now run semantic searches against the OpenSearch index. We're using `knn` as query type and scan the content of the `content_vector` field.\n",
    "\n",
    "After running the block below, we should see content semantically similar to the question. Expect documents based on Pineapples, Pizza, Hawaii, Italy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "OpenSearch is a powerful tool providing both text and vector search capabilities. Used alongside LangChain allows you to craft personalized AI applications able to augment the context based on semantic search. LangChain's extensive modularity allows you to choose your\n",
    "\n",
    "You can try Aiven for OpenSearch, or any of the other Open Source tools, in the Aiven platform free trial by [signing up](https://go.aiven.io/openai-opensearch-signup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
