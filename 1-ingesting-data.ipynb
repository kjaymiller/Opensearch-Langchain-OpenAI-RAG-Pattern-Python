{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search using OpenSearch and OpenAI\n",
    "\n",
    "This notebook guides you through an example of using [Aiven for OpenSearch](https://go.aiven.io/openai-opensearch-os) as a backend vector database for OpenAI embeddings and how to perform semantic search.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have created all necessary accounts and services as highlighted in the [README](./README.md) to follow the prerequisites:\n",
    "- You have an [Aiven Account](./README.md#setup-your-aiven-account)\n",
    "- You have created your [opensearch service](./README.md#create-an-opensearch-service)\n",
    "- You have and OpenAI Account\n",
    "- You have created AND SAVED an OpenAI API key\n",
    "- You have setup your python environment for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding our Environment Variables\n",
    "To avoid leaking api_keys we will store them in an .env file that is ignored from version control.\n",
    "\n",
    "**make a copy of `.env_sample`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "! cp .env_sample .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Add our OpenAI API key\n",
    "\n",
    "Open `.env` and replace `<YOUR_OPENAI_API_KEY>` with the key that you saved from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add our OpenSearch Service URI\n",
    "\n",
    "Verify the Aiven for OpenSearch service is in the `RUNNING` state.\n",
    "\n",
    "![OpenSearch service in the running state](./assets/opensearch-running-state.png)\n",
    "\n",
    "Select the running service and copy the **Service URI**.\n",
    "\n",
    "![Copy the OpenSearch Service URI](assets/copy-opensearch-service-uri.png)\n",
    "\n",
    "Add the OpenSearch Service URI to your `.env` file created above, replacing `<OPENSEARCH_SERVICE_URI>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load our environment variables and Connect to our Opensearch Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "connection_string = os.getenv(\"OPENSEARCH_SERVICE_URI\")\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(connection_string, use_ssl=True, timeout=100)\n",
    "\n",
    "# Verify the connection\n",
    "\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "To save us from having to recalculate embeddings on a huge dataset, we are using a pre-calculated OpenAI embeddings dataset covering wikipedia articles. We can get the file and unzip it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "\n",
    "embeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n",
    "wget.download(embeddings_url)\n",
    "\n",
    "with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\n",
    "\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains:\n",
    "* `id` a unique Wikipedia article identifier\n",
    "* `url` the Wikipedia article URL\n",
    "* `title` the title of the Wikipedia page\n",
    "* `text` the text of the article\n",
    "* `title_vector` and `content_vector` the embedding calculated on the title and content of the wikipedia article respectively\n",
    "* `vector_id` the id of the vector\n",
    "\n",
    "We can create an OpenSearch mapping optimized for this information with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings ={\n",
    "    \"index\": {\n",
    "      \"knn\": True,\n",
    "      \"knn.algo_param.ef_search\": 100\n",
    "    }\n",
    "  }\n",
    "\n",
    "index_mapping= {\n",
    "    \"properties\": {\n",
    "      \"content_vector\": {\n",
    "          \"type\": \"knn_vector\",\n",
    "          \"dimension\": 1536,\n",
    "          \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"l2\",\n",
    "            \"engine\": \"faiss\"\n",
    "        },\n",
    "      },\n",
    "      \"text\": {\"type\": \"text\"},\n",
    "      \"title\": {\"type\": \"text\"},\n",
    "      \"url\": { \"type\": \"keyword\"},\n",
    "      \"vector_id\": {\"type\": \"long\"}\n",
    "      \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an index in Aiven for OpenSearch\n",
    "\n",
    "This is where we will store our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"openai_wikipedia_index\"\n",
    "client.indices.create(index=index_name, body={\"settings\": index_settings, \"mappings\":index_mapping})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index data into OpenSearch\n",
    "\n",
    "Now it's time to parse the the pandas dataframe and index the data into OpenSearch using Bulk APIs. The following function indexes a set of rows in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "from opensearchpy import helpers\n",
    "from rich import console\n",
    "\n",
    "csv_file = open(\"data/vector_database_wikipedia_articles_embedded.csv\")\n",
    "\n",
    "def _load_data():\n",
    "        wikipedia_articles = csv.DictReader(csv_file)\n",
    "\n",
    "        for row in wikipedia_articles:\n",
    "            \n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": row['id'],\n",
    "                \"_source\": {\n",
    "                    'url' : row[\"url\"],\n",
    "                    'title' : row[\"title\"],\n",
    "                    'text' : row[\"text\"],\n",
    "                    'content_vector' : json.loads(row[\"content_vector\"]),\n",
    "                    'vector_id' : row[\"vector_id\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "csv_file.seek(0)\n",
    "succeeded = []\n",
    "failed = []\n",
    "\n",
    "\n",
    "console = console.Console()\n",
    "\n",
    "with console.status(\"Indexing data...\") as status:\n",
    "    for success, item in helpers.parallel_bulk(client, actions=_load_data()):\n",
    "        if success:\n",
    "            succeeded.append(item)\n",
    "        else:\n",
    "            failed.append(item)\n",
    "\n",
    "    if len(failed) > 0:\n",
    "        console.print(f\"There were {len(failed)} errors:\", style=\"bold red\")\n",
    "        for item in failed:\n",
    "            print(item[\"index\"][\"error\"])\n",
    "\n",
    "    if len(succeeded) > 0:\n",
    "        console.print(f\"Bulk-inserted {len(succeeded)} items (streaming_bulk).\", style=\"bold green\")\n",
    "\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Verify that our index has populated in our Aiven Console\n",
    "\n",
    "In the Aiven Console, select **Indexes** in the sidebar and verify that you have documents populated. There should be OVER 20,000 documents.\n",
    "\n",
    "![OpenSearch Indexes in the Aiven Console](assets/opensearch-indexes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the documents are indexed, let's try a query to retrieve the documents containing `Pizza`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "res = client.search(index=index_name, body={\n",
    "    \"_source\": {},\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text\": {\n",
    "                \"query\": \"Pizza\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "pprint(res[\"hits\"][\"hits\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/badge/2-Create%20Search%20with%20LangChain-48a1db?style=for-the-badge)](2-search-with-langchain.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
